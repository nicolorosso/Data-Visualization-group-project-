{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64429fa-1555-4c3d-ae3e-07ec27840d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Agreement Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2907f456-219b-4d03-aeb2-65b9b91ed833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69bb09c-40df-4eaf-95b3-f7857446c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelizza_agreement(lista_convos):\n",
    "    \"\"\" Given a list with two elements (strings), evaluates the agreement between the two strings using\n",
    "    the RoBERTa model for natural language inference and randomly prints the first string with a\n",
    "    probability of 1%.\"\"\"\n",
    "    \n",
    "    valori = []\n",
    "    \n",
    "    original = lista_convos[0]\n",
    "    reply = lista_convos[1]\n",
    "    \n",
    "    tokens = roberta.encode(original, reply)\n",
    "    \n",
    "    try:\n",
    "        valori.append(roberta.predict('mnli', tokens).argmax())  # 0: contradiction\n",
    "    except:\n",
    "        valori.append('Error')\n",
    "        \n",
    "    choice = np.random.choice(['y','n'],p=[1/100,99/100])\n",
    "    if choice=='y':\n",
    "        print(lista_convos[0])\n",
    "    \n",
    "    return valori\n",
    "\n",
    "\n",
    "def estrai_tensore(tensore):\n",
    "    try:\n",
    "        return tensore.item()\n",
    "    except:\n",
    "        return tensore\n",
    "    \n",
    "\n",
    "def remap_agreement(x):\n",
    "    \"\"\" Roberta Large MNLI maps agreements the following way:\n",
    "        0: contradiction\n",
    "        1: neutral\n",
    "        2: entailment.\n",
    "        For the sake of our analysis we will remap them as follows:\n",
    "        0: -1\n",
    "        1: +1\n",
    "        2: +1 \n",
    "        \"\"\"\n",
    "\n",
    "    agreement_mapper = {0:-1, 1:1, 2:1}\n",
    "    return agreement_mapper[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ffbefc-702f-4f86-93fc-da8f0f83b9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r'./data/dataset_original_processed/tweets_dataviz_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d1b574-fe85-4fe5-9dcd-834353db7453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_for_agreement = df[['text', 'originalTweetContent'\n",
    "                      ]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e780b6-2f3e-44b8-8c56-bee40a1e65b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lista_convos = list(zip(df_for_agreement['text'], df_for_agreement['originalTweetContent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3ab37-753a-41dc-ab40-3c274d1d5aa2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool()\n",
    "results = pool.map(parallelizza_agreement, lista_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abfb5f5-34c5-429a-99e0-34d75d9108b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = pd.DataFrame(lista_convos, columns=['text', 'originalTweetContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e976752-9dad-40a2-975a-5e2c6a0617af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint['agreement'] = results\n",
    "checkpoint.agreement = checkpoint.agreement.apply(lambda x: x[0])\n",
    "checkpoint.agreement = checkpoint.agreement.apply(estrai_tensore)\n",
    "checkpoint.agreement = checkpoint.agreement.apply(remap_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55751e8a-625d-4b88-96af-76faa67c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Error\" was our catch-all response in case roberta agreement detection failed\n",
    "# so now we drop every row containing it\n",
    "checkpoint = checkpoint[checkpoint['agreement']!='Error'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5e58da-e707-4b82-b958-83a251dcbc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint.to_json(r'./data/dataset_original_processed/tweets_dataviz_agreements.json',\n",
    "               force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704187e-7379-4376-bf1e-ad1d2fbeeffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c565c58-fe40-4ee9-8e5f-47e128010a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import os\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6689a0b-9f3e-4d3d-ae9b-8f2a192d0897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_top10_communities_distribution(G, df):\n",
    "    \"\"\" For a given graph (G) and a DataFrame (df), this function iteratively computes the modularity\n",
    "    communities using the greedy modularity algorithm with varying resolutions. It then calculates\n",
    "    the top 10 communities by size and plots their distribution for each resolution \"\"\"\n",
    "    \n",
    "    for i in range(20, 60):\n",
    "        if i%2==0 and i!=0:\n",
    "            c = greedy_modularity_communities(G, resolution=i/10)\n",
    "            a = []\n",
    "\n",
    "            for indice in range(len(c)):\n",
    "                a.append(dict(zip(list(c[indice]), (f'{str(indice)} '*len(c[indice])).split(' '))))\n",
    "\n",
    "\n",
    "            b = dict( chain( *map( dict.items, a ) ) )\n",
    "\n",
    "            df['SourceModularity'] = df[\"username\"].map(b)\n",
    "            df['TargetModularity'] = df[\"originalUsernamePost\"].map(b)\n",
    "\n",
    "            (df.TargetModularity.value_counts()[:10] + \\\n",
    "            df.SourceModularity.value_counts()[:10]).sort_values(ascending=False).plot(kind='bar')\n",
    "            plt.title(f'resolution {i/10}')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "def pick_best_resolution(G,df, resolution):\n",
    "    \"\"\"Given a graph G, a DataFrame df, and a resolution value, this function computes the greedy modularity\n",
    "    communities of the graph and assigns the modularity values to the DataFrame's 'SourceModularity' and\n",
    "    'TargetModularity' columns.\"\"\"\n",
    "    \n",
    "    community_list = greedy_modularity_communities(G, resolution=resolution)\n",
    "    community_mapping_list = []\n",
    "\n",
    "    for indice in range(len(community_list)):\n",
    "        # Create a dictionary that maps each node in the current community to the community index (indice) as a string\n",
    "        community_mapping_list.append(dict(zip(list(community_list[indice]), (f'{str(indice)} '*len(community_list[indice])).split(' '))))\n",
    "        \n",
    "    from itertools import chain\n",
    "    #  The following line merges all the dictionaries in the 'community_mapping_list' into a single dictionary called 'merged_community_mapping':\n",
    "    merged_community_mapping = dict( chain( *map( dict.items, community_mapping_list ) ) )\n",
    "    \n",
    "    df['SourceModularity'] = df[\"username\"].map(merged_community_mapping)\n",
    "    df['TargetModularity'] = df[\"originalUsernamePost\"].map(merged_community_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97615e91-ad5c-4ae9-883f-a6ff93dea663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r'./data/dataset_original_processed/tweets_dataviz_agreements.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b577d5c3-50f0-4fab-a017-5629bc484fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df, \"username\", \n",
    "                            \"originalUsernamePost\", \n",
    "                            create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b4ea1-0a52-47e9-9e20-9eacca4f109e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_top10_communities_distribution(G, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e391ca-1056-437e-adab-8b5b76edc341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pick_best_resolution(G, df, 2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea181f2-6165-4198-bbbc-cd9d38eb9e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json(r'./data/dataset_original_processed/tweets_dataviz_agreements_comms.json',\n",
    "           force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941a783-e39b-4bf3-86c4-3d6348936b9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Keywords mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00c35fb-5ca8-4638-b951-26329ea3736c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84900854-f90e-439c-94ab-374bbf9bf434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kw_yake(x):\n",
    "    try:\n",
    "        kw_and_confidence = custom_kw_extractor.extract_keywords(x)\n",
    "        kw = kw_and_confidence[0][0]\n",
    "        return kw\n",
    "    except:\n",
    "        return 'KEYWORD_NOT_FOUND'\n",
    "    \n",
    "    \n",
    "def rimuovi_prima_parola(df):\n",
    "    \"\"\"A further text processing step is removing all the keywords that are nothing but the\n",
    "    username of the user that posts the tweet\"\"\"\n",
    "    \n",
    "    username_to_remove = df['originalUsernamePost']\n",
    "    topkw = df['TopKeyword']\n",
    "    \n",
    "    topkw = topkw.split()\n",
    "    if topkw[0] == username_to_remove:\n",
    "        return \" \".join(topkw[1:])\n",
    "    else:\n",
    "        return \" \".join(topkw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10334d9-da0c-4cad-93e5-2ffcf526fbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r'./data/dataset_original_processed/tweets_dataviz_agreements_comms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294798bb-8ab0-4eac-9b43-77cf693668c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "max_ngram_size = 2\n",
    "numOfKeywords = 1\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,\n",
    "                                            top=numOfKeywords, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed01f2a-c460-4cf5-a546-caa23ecd9d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kw_of_unique_texts = pd.Series(df['text'].unique()).apply(extract_kw_yake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ec9d36-75a0-4ff3-b01f-27cb86b676e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_kw_mapper = dict(zip(df['text'].unique().tolist(), \n",
    "         kw_of_unique_texts.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df45aea-2278-4f18-ab71-c9e9fab4cc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['TopKeyword'] = df['text'].map(text_kw_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b189574c-e498-48f3-a371-7ec7cce04467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df['TopKeyword']!='KEYWORD_NOT_FOUND'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15a6c01f-416e-4595-abf9-f4bb320bfdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['TopKeyword'] = df.apply(rimuovi_prima_parola,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76c9c441-2f5c-4a40-8930-f1f6e1880382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# but now the tweets that had as a keyword just the username display an empty cell,\n",
    "# let's drop it\n",
    "\n",
    "df = df.drop(df.loc[df['TopKeyword'] == ''].index,\n",
    "       axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62f6ef14-dbc2-4603-8912-65f857a0c094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json(r'./data/dataset_original_processed/tweets_dataviz_agreements_comms_kw.json',\n",
    "           force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29806ce-e745-4450-90d1-521bd41eb84f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Edge Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc15964-3e86-46ef-beae-cf32b3ddcb69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef0b975-6d07-4514-a9b5-081a3872b290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calcola_edge_betweenness(df):\n",
    "    G = nx.from_pandas_edgelist(df, 'SourceModularity','TargetModularity')\n",
    "    bet = nx.edge_betweenness_centrality(G)\n",
    "\n",
    "    dfbet = pd.DataFrame(bet.keys(),columns=['SourceModularity','TargetModularity'])\n",
    "    dfbet['edge_bet'] = bet.values()\n",
    "\n",
    "    df = pd.merge(df,dfbet, on=['SourceModularity','TargetModularity'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd39a1c0-80c8-46a6-8322-13fb26da3073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r'./data/dataset_original_processed/tweets_dataviz_agreements_comms_kw.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef1fe88-7ce4-45e5-9cb4-cf6787a1c78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = calcola_edge_betweenness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb99fa1e-6550-49e5-ae10-ff40b1ed68b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json(r'./data/dataset_original_processed/tweets_dataviz_agreements_comms_kw_bet.json',\n",
    "           force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6aba7-66e3-4e1c-8473-571855ecaa31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset splitting in subsets by research keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b877a376-e560-41d2-9455-0545612fa598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989dd379-ea71-4bc9-be60-b201030f7a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_top10_communities_distribution(G, df):\n",
    "    for i in range(12, 42):\n",
    "        if i%2==0 and i!=0:\n",
    "            c = greedy_modularity_communities(G, resolution=i/10)\n",
    "            a = []\n",
    "\n",
    "            for indice in range(len(c)):\n",
    "                a.append(dict(zip(list(c[indice]), (f'{str(indice)} '*len(c[indice])).split(' '))))\n",
    "\n",
    "\n",
    "            b = dict( chain( *map( dict.items, a ) ) )\n",
    "\n",
    "            df['SourceModularity'] = df[\"username\"].map(b)\n",
    "            df['TargetModularity'] = df[\"originalUsernamePost\"].map(b)\n",
    "\n",
    "            (df.TargetModularity.value_counts()[:10] + \\\n",
    "            df.SourceModularity.value_counts()[:10]).sort_values(ascending=False).plot(kind='bar')\n",
    "            plt.title(f'resolution {i/10}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f524e7-3df1-4581-b3bb-8b84588f13b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pick_best_resolution(G,df, resolution):\n",
    "    c = greedy_modularity_communities(G, resolution=resolution)\n",
    "    a = []\n",
    "\n",
    "    for indice in range(len(c)):\n",
    "        a.append(dict(zip(list(c[indice]), (f'{str(indice)} '*len(c[indice])).split(' '))))\n",
    "        \n",
    "    from itertools import chain\n",
    "    b = dict( chain( *map( dict.items, a ) ) )\n",
    "    \n",
    "    df['SourceModularity'] = df[\"username\"].map(b)\n",
    "    df['TargetModularity'] = df[\"originalUsernamePost\"].map(b)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e345551d-4da4-491f-bcda-f30ef23a8a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calcola_edge_betweenness(df):\n",
    "    G = nx.from_pandas_edgelist(df, 'SourceModularity','TargetModularity')\n",
    "    bet = nx.edge_betweenness_centrality(G)\n",
    "\n",
    "    dfbet = pd.DataFrame(bet.keys(),columns=['SourceModularity','TargetModularity'])\n",
    "    dfbet['edge_bet'] = bet.values()\n",
    "\n",
    "    df = pd.merge(df,dfbet, on=['SourceModularity','TargetModularity'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c8fa19-15ca-49eb-8265-0ba2e3aa4b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\n",
    "    r'./data/dataset_original_processed/tweets_dataviz_agreements_comms_kw_bet.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0064bb-ab8e-4dfb-9eea-4a098acbb976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chosen_keywords_for_research = [\n",
    "    'elections', \n",
    "    'fraud', \n",
    "    'democrats',\n",
    "    'riggedelection', \n",
    "    'sharpiegate',\n",
    "    'capitol hill', \n",
    "    'biden', \n",
    "    'trump', \n",
    "    'georgia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a52f4-9b75-4775-958c-e775e4d63b73",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for kw in chosen_keywords_for_research:\n",
    "    print(f\"SUBSET {kw}: {df[(df.text.str.lower()).str.contains(kw)].shape}\")\n",
    "    df_subset = df.copy()\n",
    "    df_subset = df_subset.drop('edge_bet',axis=1)\n",
    "    df_subset = df_subset[(df_subset.text.str.lower()).str.contains(kw)]\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(df_subset, \"username\", \n",
    "                            \"originalUsernamePost\", \n",
    "                            create_using=nx.DiGraph)\n",
    "    \n",
    "    plot_top10_communities_distribution(G, df_subset) \n",
    "    best_resol = input(f'Pick a resolution for {kw}: ')\n",
    "    best_resol = float(best_resol)\n",
    "    df_subset = pick_best_resolution(G, df_subset, best_resol)\n",
    "    \n",
    "    df_subset = calcola_edge_betweenness(df_subset)\n",
    "    \n",
    "    df_subset.to_json(rf'./data/datasets_splitted_by_research_kw/{kw}.json',\n",
    "               force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50afc4b-a04c-4b03-994a-d3042b343058",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistical Analyses on subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dcc8a5a-1b03-47c0-9f40-306fe8dfa30d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01dca76-5599-4bdc-9205-9329110ef695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StatisticalAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.top10comms = (df.TargetModularity.value_counts()[:10] + \\\n",
    "                           df.SourceModularity.value_counts()[:10]) \\\n",
    "                           .sort_values(ascending=False).index.tolist()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_chi2(column_x, column_y):\n",
    "        \"\"\"Computes the chi-square test statistic, p-value, degrees of freedom, and expected frequencies \n",
    "        using the observed frequencies of occurrence of the events described by two columns of a DataFrame.\n",
    "        Parameters:\n",
    "            column_x (pandas.Series): A categorical pandas Series, which will be used as rows in the contingency table.\n",
    "            column_y (pandas.Series): A categorical pandas Series, which will be used as columns in the contingency table.\"\"\"\n",
    "        return stats.chi2_contingency(pd.crosstab(column_x, column_y))\n",
    "    \n",
    "    \n",
    "    def a1(self):\n",
    "        \"\"\" Adds a new column to the dataframe, SameCommunitySourceTarget, which is the integer representation of whether 'SourceModularity' \n",
    "        and 'TargetModularity' are the same.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        df['SameCommunitySourceTarget'] = (df['SourceModularity'] == df['TargetModularity']).astype('int')        \n",
    "        return df\n",
    "    \n",
    "    def a2(self):\n",
    "        \"\"\" Filters the DataFrame by selecting only the rows where the 'SourceModularity' and 'TargetModularity' \n",
    "        are the same. \n",
    "        The top 10 communities are considered separately, and the rest are grouped under \n",
    "        the same category (11). \"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        \n",
    "        df.loc[(df['SourceModularity']==df['TargetModularity']) \n",
    "               & (~df['SourceModularity'].isin(top10comms)), 'SourceModularity'] = 11\n",
    "        df.loc[(df['SourceModularity']==df['TargetModularity']) \n",
    "               & (~df['TargetModularity'].isin(top10comms)), 'TargetModularity'] = 11\n",
    "        return df\n",
    "    \n",
    "    def a3(self):\n",
    "        \"\"\" Filters the DataFrame by selecting only the rows where the 'SourceModularity' and 'TargetModularity' \n",
    "        are different, and then creates a new column called 'InterCommunities' which represents the interaction \n",
    "        between two communities. The top 10 communities are considered separately, and the rest are grouped under \n",
    "        the same category (11). \"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        \n",
    "        df.loc[(df['SourceModularity']!=df['TargetModularity']) \n",
    "                & (~df['SourceModularity'].isin(top10comms)), 'SourceModularity'] = 11\n",
    "\n",
    "        df.loc[(df['SourceModularity']!=df['TargetModularity']) \n",
    "               & (~df['TargetModularity'].isin(top10comms)), 'TargetModularity'] = 11\n",
    "        \n",
    "        df = df.loc[df['SourceModularity']!=df['TargetModularity']].reset_index(drop=True)\n",
    "        df['InterCommunities'] = df['SourceModularity'].astype('str') +  '-' + df['TargetModularity'].astype('str')\n",
    "        return df\n",
    "    \n",
    "    def a4(self):\n",
    "        \"\"\"Filters the DataFrame by selecting only the rows where:\n",
    "        1. The 'SourceModularity' and 'TargetModularity' are the same and both are in the top 10 communities, or\n",
    "        2. The 'SourceModularity' and 'TargetModularity' are different and both are in the top 10 communities.\n",
    "        Then, it creates a new column called 'InterCommunities' which represents the interaction between\n",
    "        the two communities.\"\"\"\n",
    "\n",
    "\n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        \n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df.loc[(df['SourceModularity']==df['TargetModularity']) & \n",
    "                       (df['SourceModularity'].isin(top10comms))][['SourceModularity','TargetModularity','agreement']], \n",
    "\n",
    "                df.loc[(df['SourceModularity'].isin(top10comms)) & \n",
    "                       (df['SourceModularity']!=df['TargetModularity'])][['SourceModularity','TargetModularity','agreement']\n",
    "                                                                        ]\n",
    "            ]\n",
    "        )\n",
    "        df['InterCommunities'] = df['SourceModularity'].astype('str') +  '-' + df['TargetModularity'].astype('str')\n",
    "        return df\n",
    "    \n",
    "    def a5(self):\n",
    "        \"\"\" Computes the chi-square test between the most frequent 100 keywords and\n",
    "        agreement/disagreement in the dataset. \n",
    "        The method first groups the DataFrame by 'TopKeyword' and 'agreement' columns and calculates the size of each group. \n",
    "        Then, it selects the top 100 keywords based on their total frequency in the dataset and computes the chi-square test. \n",
    "        The p-value of the test is returned.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        df = df.groupby(['TopKeyword', 'agreement']).size().unstack(fill_value=0).reset_index()\n",
    "        df['sum'] = df[[-1,1]].sum(axis=1)\n",
    "        df = df.sort_values('sum',ascending=False).reset_index(drop=True).drop(['sum'],axis=1)\n",
    "        df = df.head(100)\n",
    "        df = df.set_index('TopKeyword')\n",
    "        return stats.chi2_contingency(df)[1]   \n",
    "    \n",
    "    def a6(self):\n",
    "        \"\"\"Computes the chi-square test between keywords and agreement/disagreement within each of the top 10 communities in the dataset. \n",
    "        For each community, the method filters the DataFrame to include only rows where both 'SourceModularity' and 'TargetModularity' match the community, and groups the filtered data by 'TopKeyword' and 'agreement'. \n",
    "        Then, it calculates the size of each group and performs the chi-square test for the top 10 keywords in each community. \"\"\"        \n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        for i in top10comms:\n",
    "            kw = df.loc[(df['SourceModularity']==i) & \n",
    "                        (df['TargetModularity']==i) & \n",
    "                        (df['agreement']!='error')].groupby(['TopKeyword', 'agreement']).size().unstack(fill_value=0)\n",
    "            try:\n",
    "                vals = {}\n",
    "                kw['sum'] = kw[[-1,1]].sum(axis=1)\n",
    "                kw = kw.sort_values('sum',ascending=False).head(10)\n",
    "                kw = kw[[-1,1]]\n",
    "                vals[f\"Comunità {i}-{i}\"] = stats.chi2_contingency(kw)[1]\n",
    "            except:\n",
    "                vals[f\"Comunità {i}-{i}\"] = None\n",
    "        \n",
    "    def a7(self):\n",
    "        \"\"\"Computes the chi-square test between keywords connecting different communities and agreement/disagreement across the entire dataset. \n",
    "        The method filters the DataFrame to include only rows where 'SourceModularity' and 'TargetModularity' are different.\n",
    "        Then, it creates a new column 'SKT' that combines 'SourceModularity', 'TopKeyword',and 'TargetModularity'. \n",
    "        It calculates the cross-tabulation of 'SKT' and 'agreement' and performs the chi-square test on this cross-tabulation. \n",
    "        The p-value of the test is returned\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        \n",
    "        df = df.loc[(df['SourceModularity']!=df['TargetModularity']) & (df['agreement']!='Error')] \\\n",
    "        .assign(SKT=df['SourceModularity'].astype('str') + '-' + df['TopKeyword'] + '-' + df['TargetModularity'].astype('str'))\n",
    "\n",
    "        kw = pd.crosstab(df['SKT'], df['agreement'])\n",
    "        try:\n",
    "            kw['somma'] = kw[[-1,1]].sum(axis=1)\n",
    "            kw = kw.sort_values('somma', ascending=False)[[-1,1]]\n",
    "\n",
    "            return stats.chi2_contingency(kw)[1]\n",
    "        except: return None\n",
    "    \n",
    "    def a8(self):\n",
    "        \"\"\"Filters the DataFrame to include only rows where 'SourceModularity' and 'TargetModularity'are different and both 'SourceModularity' and 'TargetModularity'are in the top 10 communities. \n",
    "        Then, it creates a new column 'SKT' that combines 'SourceModularity', 'TopKeyword', and 'TargetModularity'. \n",
    "        The resulting DataFrame is returned.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        \n",
    "        df = df.loc[(df['SourceModularity']!=df['TargetModularity']) & (df['agreement']!='Error')] \\\n",
    "        .assign(SKT=df['SourceModularity'].astype('str') + '-' + df['TopKeyword'] + '-' + df['TargetModularity'].astype('str'))\n",
    "\n",
    "        df = df.loc[(df['SourceModularity'].isin(top10comms)) & \n",
    "                    (df['TargetModularity'].isin(top10comms)) & \n",
    "                    (df['SourceModularity']!=df['TargetModularity'])]\n",
    "        return df\n",
    "\n",
    "    def a9(self):\n",
    "        \"\"\"Filters the DataFrame to include only rows where 'SourceModularity' and 'TargetModularity'are different and both 'SourceModularity' and 'TargetModularity' are in the top 10 communities. \n",
    "        Then, it creates a new column 'SKT' that combines 'SourceModularity', 'TopKeyword', and 'TargetModularity'. \n",
    "        For each top 10 community, select the top 10 rows with 'SKT' that starts with the community's number, and concatenate the results. \"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "\n",
    "        df = df.loc[(df['SourceModularity']!=df['TargetModularity']) & (df['agreement']!='Error')] \\\n",
    "        .assign(SKT=df['SourceModularity'].astype('str') + '-' + df['TopKeyword'] + '-' + df['TargetModularity'].astype('str'))\n",
    "\n",
    "\n",
    "        df = df.loc[(df['SourceModularity'].isin(top10comms)) & \n",
    "                       (df['TargetModularity'].isin(top10comms)) & \n",
    "                       (df['SourceModularity']!=df['TargetModularity'])][['SKT','agreement']]\n",
    "\n",
    "        dfs = []\n",
    "        for comm in top10comms:\n",
    "            dfs.append(df[df.SKT.str.startswith(f\"{comm}-\")].head(10))\n",
    "        dfs = pd.concat(dfs)\n",
    "\n",
    "        return dfs\n",
    "    \n",
    "    def a10(self):\n",
    "        \"\"\"Filters the DataFrame to include rows where 'SourceModularity' is in the top 10 communities and 'TargetModularity' is not in the top 10 communities. \n",
    "        Then, it creates a new column 'SKT' that combines 'SourceModularity', 'TopKeyword', and 'TargetModularity'. \n",
    "        The method computes a crosstab of 'SKT' and 'agreement', then sorts the result by the sum of agreement and disagreement counts and removes any rows with all zeros.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "\n",
    "        df['SKT'] = df['SourceModularity'].astype('str') + '-' + df['TopKeyword'] + '-' + df['TargetModularity'].astype('str')\n",
    "\n",
    "        df = pd.crosstab(df.loc[(df['SourceModularity'].isin(top10comms)) & (~df['TargetModularity'].isin(top10comms))]['SKT'],\n",
    "                         df.loc[(df['SourceModularity'].isin(top10comms)) & (~df['TargetModularity'].isin(top10comms))]['agreement'])\n",
    "    \n",
    "        try:\n",
    "            df['somma'] = df[[-1,1]].sum(axis=1)\n",
    "            df = df.loc[(df!=0).any(axis=1)].sort_values('somma',ascending=False).drop('somma',axis=1)\n",
    "            return df\n",
    "        except: return None\n",
    "    \n",
    "    def a11(self):\n",
    "        \"\"\"For each community in the top 10 communities, this method filters the DataFrame to select rows\n",
    "        where both 'SourceModularity' and 'TargetModularity' are equal to the current community index.\n",
    "        Then, it separates the 'edge_bet' values based on whether the 'agreement' column has a value of 1\n",
    "        (agreement) or -1 (disagreement). \n",
    "        Finally, it computes the Kruskal-Wallis H-test on these two groups\n",
    "        of 'edge_bet' values and prints the community index along with the test results.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        for i in top10comms:\n",
    "            temp = df.loc[(df['SourceModularity']==i) & (df['TargetModularity']==i)]\n",
    "            edge_bet_agreement = temp.loc[temp['agreement']==1]['edge_bet']\n",
    "            edge_bet_disagreement = temp.loc[temp['agreement']==-1]['edge_bet']\n",
    "            print(i,stats.kruskal(edge_bet_agreement.values, edge_bet_disagreement.values))\n",
    "            \n",
    "    def a12(self):\n",
    "        \"\"\"Filters the DataFrame to select rows where 'SourceModularity' is in the top 10 communities,\n",
    "        and 'TargetModularity' is not in the top 10 communities. Then, it separates the 'edge_bet'\n",
    "        values based on whether the 'agreement' column has a value of 1 (agreement) or -1 (disagreement).\n",
    "        Finally, it computes the Kruskal-Wallis H-test on these two groups of 'edge_bet' values and returns\n",
    "        the test results\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        df = df.loc[(df['SourceModularity'].isin(top10comms)) & (~df['TargetModularity'].isin(top10comms))]\n",
    "        edge_bet_agreement = df.loc[df['agreement']==1]['edge_bet']\n",
    "        edge_bet_disagreement = df.loc[df['agreement']==-1]['edge_bet']\n",
    "        return stats.kruskal(edge_bet_agreement.values, edge_bet_disagreement.astype('int').values)\n",
    "    \n",
    "    def a13(self):\n",
    "        \"\"\"Modifies the DataFrame by setting 'SourceModularity' and 'TargetModularity' to 999 for all rows\n",
    "        where the respective modularity value is not in the top 10 communities. Then, it separates the\n",
    "        'edge_bet' values based on whether the 'agreement' column has a value of 1 (agreement) or -1\n",
    "        (disagreement). \n",
    "        Finally, it computes the Kruskal-Wallis H-test on these two groups of 'edge_bet' values and returns the test results.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "        df.loc[~df['SourceModularity'].isin(top10comms), 'SourceModularity'] = 999\n",
    "        df.loc[~df['TargetModularity'].isin(top10comms), 'TargetModularity'] = 999\n",
    "        edge_bet_agreement = df.loc[df['agreement']==1]['edge_bet']\n",
    "        edge_bet_disagreement = df.loc[df['agreement']==-1]['edge_bet']\n",
    "        return stats.kruskal(edge_bet_agreement.values, edge_bet_disagreement.astype('int').values)\n",
    "\n",
    "    def a14(self):\n",
    "        \"\"\"Plots the quantile function of 'edge_bet' values for agreement and disagreement cases.\n",
    "        The method filters the DataFrame to separate the 'edge_bet' values based on whether the\n",
    "        'agreement' column has a value of 1 (agreement) or -1 (disagreement). \n",
    "        Then, it computes the percentiles for both groups and plots the quantile functions for the 80th to 100th\n",
    "        percentiles on a logarithmic scale.\"\"\"\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        top10comms = self.top10comms\n",
    "\n",
    "        edges_agreement = df.loc[df.agreement==1].edge_bet.values\n",
    "        edges_disagreement = df.loc[df.agreement==-1].edge_bet.values\n",
    "\n",
    "        percs = np.linspace(0,100,500)\n",
    "\n",
    "        qn_edges_agreement = np.percentile(edges_agreement, percs)\n",
    "        qn_edges_disagreement = np.percentile(edges_disagreement, percs)\n",
    "\n",
    "        plt.plot(percs[400:],qn_edges_agreement[400:])\n",
    "        plt.plot(percs[400:],qn_edges_disagreement[400:])\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e97a937-d1a1-4ee6-abdb-02a491455392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/datasets_splitted_by_research_kw/biden.json\n",
      "./data/datasets_splitted_by_research_kw/capitol hill.json\n",
      "./data/datasets_splitted_by_research_kw/democrats.json\n",
      "./data/datasets_splitted_by_research_kw/elections.json\n",
      "./data/datasets_splitted_by_research_kw/fraud.json\n",
      "./data/datasets_splitted_by_research_kw/georgia.json\n",
      "./data/datasets_splitted_by_research_kw/trump.json\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "PATH = './data/datasets_splitted_by_research_kw'\n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith('.json'):\n",
    "        print(f\"{PATH}/{file}\")\n",
    "        df_subset = pd.read_json(f\"{PATH}/{file}\")\n",
    "        sa = StatisticalAnalyzer(df_subset)\n",
    "        _, pvalue1, _, _ = sa.compute_chi2(sa.a1()['SameCommunitySourceTarget'], sa.a1()['agreement'])\n",
    "        _, pvalue2, _, _ = sa.compute_chi2(sa.a2()['SourceModularity'], sa.a2()['agreement'])\n",
    "        _, pvalue3, _, _ = sa.compute_chi2(sa.a3()['InterCommunities'], sa.a3()['agreement'])\n",
    "        _, pvalue4, _, _ = sa.compute_chi2(sa.a4()['InterCommunities'], sa.a4()['agreement'])\n",
    "\n",
    "\n",
    "        _, pvalue4_1, _, _ = sa.compute_chi2(\n",
    "            sa.a4()[sa.a4()['SourceModularity']==sa.a4()['TargetModularity']]['SourceModularity'],\n",
    "            sa.a4()[sa.a4()['SourceModularity']==sa.a4()['TargetModularity']]['agreement']\n",
    "        )\n",
    "\n",
    "\n",
    "        _, pvalue4_2, _, _ = sa.compute_chi2(\n",
    "            sa.a4()[sa.a4()['SourceModularity']!=sa.a4()['TargetModularity']]['SourceModularity'],\n",
    "            sa.a4()[sa.a4()['SourceModularity']!=sa.a4()['TargetModularity']]['agreement']\n",
    "        )\n",
    "\n",
    "        pvalues4_3 = []\n",
    "        for comm in sa.top10comms:\n",
    "            temp = df_subset.loc[(df_subset['SourceModularity']==comm)][['SourceModularity','TargetModularity','agreement']]\n",
    "\n",
    "            temp['SameCommunitySourceTarget'] = (temp['SourceModularity'] == temp['TargetModularity']).astype('int')\n",
    "\n",
    "            crosstab = pd.crosstab(temp['SameCommunitySourceTarget'], temp['agreement'])\n",
    "\n",
    "            _, pvalue4_3, _, _ = stats.chi2_contingency(crosstab)\n",
    "            pvalues4_3.append(pvalue4_3)\n",
    "\n",
    "        pvalue5 = sa.a5()\n",
    "        pvalue6 = sa.a6()\n",
    "        pvalue7 = sa.a7()\n",
    "        _,pvalue8,_,_ = sa.compute_chi2(sa.a8()['SKT'], sa.a8()['agreement'])\n",
    "        _,pvalue9,_,_ = sa.compute_chi2(sa.a9()['SKT'], sa.a9()['agreement'])\n",
    "        _,pvalue10,_,_ = stats.chi2_contingency(sa.a10())\n",
    "        pvalue12 = sa.a12()\n",
    "        pvalue13 = sa.a13()\n",
    "        \n",
    "        page = pd.DataFrame({'A1': pvalue1,\n",
    "                     'A2':pvalue2,\n",
    "                     'A3': pvalue3,\n",
    "                     'A4': pvalue4,\n",
    "                     'A4.1':pvalue4_1,\n",
    "                     'A4.2': pvalue4_2,\n",
    "                     'A4.3': [pvalues4_3],\n",
    "                     'A5': pvalue5,\n",
    "                     'A6': pvalue6,\n",
    "                     'A7': pvalue7,\n",
    "                     'A8': pvalue8,\n",
    "                     'A9': pvalue9,\n",
    "                     'A10': pvalue10,\n",
    "                     'A12': pvalue12.pvalue,\n",
    "                     'A13': pvalue13.pvalue}, \n",
    "                           index=[file])\n",
    "        dfs.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951a19f9-2e57-42c0-b6cd-737cb27116f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.concat(dfs).to_excel('summary_statistical_analyses.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
